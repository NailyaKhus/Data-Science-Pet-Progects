{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-20T21:09:27.470173Z",
     "iopub.status.busy": "2022-02-20T21:09:27.469035Z",
     "iopub.status.idle": "2022-02-20T21:09:28.267891Z",
     "shell.execute_reply": "2022-02-20T21:09:28.266767Z",
     "shell.execute_reply.started": "2022-02-20T21:09:27.470042Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных 54 магазина и 33 семейства продуктов.                     \n",
    "Временной ряд тренировочных данных с 01.01.2013 по 15.08.2017.              \n",
    "Временной ряд тестовых данных для submition составляет 16 дней после последней даты тренировочных данных: с 16.08.2017 по 31.08.2017.                   \n",
    "                                 \n",
    "Нужно составить прогноз продаж для каждого из семейств продуктов в каждом из магазинов.     \n",
    "                             \n",
    "В отдельном ноутбуке проведен Feature engineering и сформированы необходимые датасеты.\n",
    "                                \n",
    "**Текущие датасеты**                   \n",
    "1. featured_data - объединенные данные test и train с новыми признаками.\n",
    "2. zero_prediction - данные тех товаров, которые не продавались в конкретном магазине с начала 2013 года, исходя из чего можно предположить, что данные товары не будут продаваться в ближайшие 16 дней. Этот датасет мы будем объединять с предсказанными данными перед отправкой в submit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T21:09:28.270402Z",
     "iopub.status.busy": "2022-02-20T21:09:28.270103Z",
     "iopub.status.idle": "2022-02-20T21:09:31.828041Z",
     "shell.execute_reply": "2022-02-20T21:09:31.827028Z",
     "shell.execute_reply.started": "2022-02-20T21:09:28.270368Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# PACF - ACF\n",
    "# ------------------------------------------------------\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# DATA VISUALIZATION\n",
    "# ------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "warnings.filterwarnings('ignore')\n",
    "from warnings import simplefilter\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T21:09:31.829605Z",
     "iopub.status.busy": "2022-02-20T21:09:31.829373Z",
     "iopub.status.idle": "2022-02-20T21:09:59.665105Z",
     "shell.execute_reply": "2022-02-20T21:09:59.664227Z",
     "shell.execute_reply.started": "2022-02-20T21:09:31.829579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "df = pd.read_csv(\"../input/featured-data/featured_data.csv\")\n",
    "zero_prediction = pd.read_csv(\"../input/zero-prediction/zero_prediction.csv\")\n",
    "\n",
    "# Datetime\n",
    "df[\"date\"] = pd.to_datetime(df.date)\n",
    "\n",
    "zero_prediction = zero_prediction.set_index(['store_nbr', 'family', 'date']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T21:09:59.666787Z",
     "iopub.status.busy": "2022-02-20T21:09:59.666545Z",
     "iopub.status.idle": "2022-02-20T21:10:01.217615Z",
     "shell.execute_reply": "2022-02-20T21:10:01.216699Z",
     "shell.execute_reply.started": "2022-02-20T21:09:59.666759Z"
    }
   },
   "outputs": [],
   "source": [
    "d_train = df[df.date<'2017-08-16'].copy()\n",
    "d_test = df[df.date>='2017-08-16'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Критерий Дикки-Фуллера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T21:10:01.220381Z",
     "iopub.status.busy": "2022-02-20T21:10:01.219899Z",
     "iopub.status.idle": "2022-02-20T21:10:01.226785Z",
     "shell.execute_reply": "2022-02-20T21:10:01.225807Z",
     "shell.execute_reply.started": "2022-02-20T21:10:01.220342Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Подсчет критерия Дикки-Фуллера для каждого из семейства товаров в каждом из магазинов\n",
    "\n",
    "# # для экономии расчетного времени данный код закомментирован\n",
    "# # не стационарные ряды собраны в fuller_not_stat\n",
    "\n",
    "# # ряды, где недостаточно данных для подсчета критерия Дикки-Фуллера, полностью совпадают с \n",
    "# # комбинациями из zero_prediction.\n",
    "\n",
    "# fuller_not_stat = pd.DataFrame(columns = ['store_nbr', 'family'])\n",
    "# small_data = pd.DataFrame(columns = ['store_nbr', 'family'])\n",
    "\n",
    "# for num, store in enumerate(tqdm(d_train.store_nbr.unique())):\n",
    "#     for numf, fam in enumerate(tqdm(d_train.family.unique())):\n",
    "#         a=d_train[(d_train.store_nbr == store) & (d_train.family == fam)]\n",
    "#         try:\n",
    "#             adftest = sm.tsa.adfuller(a.sales)\n",
    "#             print('Магазин: ', store, 'семейство товаров: ', fam)\n",
    "#             print('adf: ', adftest[0])\n",
    "#             print('p-value: ', adftest[1])\n",
    "#             print('Critical values: ', adftest[4])\n",
    "#             if adftest[0]> adftest[4]['5%']: \n",
    "#                 print('есть единичные корни, ряд не стационарен')\n",
    "#                 fuller_not_stat = fuller_not_stat.append({'store_nbr':store, 'family':fam}, ignore_index=True)\n",
    "#                 print('___'*20)\n",
    "#             else:\n",
    "#                 print('единичных корней нет, ряд стационарен')\n",
    "#                 print('___'*20)\n",
    "#         except ValueError:\n",
    "#                 print('Магазин: ', store, 'семейство товаров: ', fam)\n",
    "#                 print('Количество данных:', len(a))\n",
    "#                 small_data = small_data.append({'store_nbr':store, 'family':fam}, ignore_index=True)\n",
    "#                 print('___'*20)\n",
    "#                 continue\n",
    "                \n",
    "# # OUTPUT EXAMPLE:                \n",
    "# # Магазин:  1 семейство товаров:  AUTOMOTIVE\n",
    "# # adf:  -4.078623538152452\n",
    "# # p-value:  0.001050394088497745\n",
    "# # Critical values:  {'1%': -3.4342954463097706, '5%': -2.8632826898390484, '10%': -2.5676977663666714}\n",
    "# # единичных корней нет, ряд стационарен            \n",
    "# fuller_not_stat.to_csv('fuller_not_stat.csv', index=False)\n",
    "# small_data.to_csv('small_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нестационарных рядов оказалось достаточно первой разности для приведения ряда к стационарному."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T21:10:01.228253Z",
     "iopub.status.busy": "2022-02-20T21:10:01.227969Z",
     "iopub.status.idle": "2022-02-20T21:10:01.252225Z",
     "shell.execute_reply": "2022-02-20T21:10:01.251291Z",
     "shell.execute_reply.started": "2022-02-20T21:10:01.228220Z"
    }
   },
   "outputs": [],
   "source": [
    "fuller_result = pd.read_csv(\"../input/fuller-result/fuller_not_stat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T21:10:01.254885Z",
     "iopub.status.busy": "2022-02-20T21:10:01.254310Z",
     "iopub.status.idle": "2022-02-20T21:10:01.262678Z",
     "shell.execute_reply": "2022-02-20T21:10:01.261882Z",
     "shell.execute_reply.started": "2022-02-20T21:10:01.254833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Для нестационарных рядов делаем список из комбинаций store + family\n",
    "fuller_list = fuller_result.values\n",
    "s_fuller_list=[]\n",
    "for i in range(fuller_list.shape[0]):\n",
    "    s_fuller_list.append(str(fuller_list[i][0]) + ',' + fuller_list[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скользящее среднее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T21:10:01.264888Z",
     "iopub.status.busy": "2022-02-20T21:10:01.264155Z"
    }
   },
   "outputs": [],
   "source": [
    "for num, store in enumerate(tqdm(d_train.store_nbr.unique())):\n",
    "    for numf, fam in enumerate(tqdm(d_train.family.unique())):\n",
    "        try:\n",
    "            a=d_train[(d_train.store_nbr == store) & (d_train.family == fam)].copy()\n",
    "            a=a.sales\n",
    "            b = str(store)+','+fam\n",
    "            if b in s_fuller_list:\n",
    "                a = a.diff().dropna() # если ряд не стационарный, то берем для него первую разность\n",
    "            # d_train.loc[(d_train.store_nbr == store) & (d_train.family == fam),\"moving_avg\"] = a.rolling(7).mean()\n",
    "            d_test.loc[(d_test.store_nbr == store) & (d_test.family == fam),'moving_avg']=a.rolling(7).mean().iloc[-1]\n",
    "            #print('store:',store,'fam:',fam)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit for moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = d_test.set_index(['store_nbr', 'family', 'date']).sort_index()\n",
    "for i in zero_prediction.index:\n",
    "    d_test.loc[i,'moving_avg']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test=d_test.reset_index()\n",
    "y_submit = d_test[['id','moving_avg']].copy()\n",
    "y_submit.columns = ['id', 'sales']\n",
    "y_submit.to_csv('submission_moving_avg.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результат на kaggle 1.90138**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA зависит от значений p, d, q:\n",
    "   - p = количество лагов, параметр AR;\n",
    "   - d = порядок разности;\n",
    "   - q = количество запаздывающих ошибок прогноза, параметр MA.\n",
    "aic (информационный критерий Акаике):\n",
    "- более низкий aic указывает на лучшую модель;\n",
    "- aic выбирет простые модели с более низким порядком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перебор параметров оказался "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = range(0, 10)\n",
    "d=1\n",
    "qs = range(0, 10)\n",
    "# Ps = range(0, 5)\n",
    "# D=1\n",
    "# Qs = range(0, 1)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "parameters = product(ps, qs)#, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Подбор параметров p и q для модели ARIMA\n",
    "all_result_table = pd.DataFrame()\n",
    "simplefilter(\"ignore\")  # ignore warnings to clean up output cells\n",
    "\n",
    "for num, store in enumerate(tqdm(d_train.store_nbr.unique())):\n",
    "    for numf, fam in enumerate(tqdm(d_train.family.unique())):\n",
    "        a=d_train[(d_train.store_nbr == store) & (d_train.family == fam)].copy()\n",
    "        a=a.set_index('date')\n",
    "        b = str(store)+','+fam\n",
    "        if b in s_fuller_list:\n",
    "            a.sales = a.sales.diff().dropna() # если ряд не стационарный, то берем для него первую разность\n",
    "        results = []\n",
    "        best_aic = float(\"inf\")\n",
    "        if len(a)!=0:\n",
    "            for param in tqdm(parameters_list):\n",
    "                try:\n",
    "                    model=sm.tsa.arima.ARIMA(a.sales, order=(param[0], d, param[1]), enforce_stationarity=False).fit()#, \n",
    "                                        #seasonal_order=(param[3], D, param[3], 24*7)).fit(disp=0)\n",
    "                #выводим параметры, на которых модель не обучается и переходим к следующему набору\n",
    "                except ValueError:\n",
    "                    print('wrong parameters:', param)\n",
    "                    continue\n",
    "                aic = model.aic\n",
    "                #сохраняем лучшую модель, aic, параметры\n",
    "                if aic < best_aic:\n",
    "                    best_model = model\n",
    "                    best_aic = aic\n",
    "                    best_param = param\n",
    "                results.append([param, model.aic])\n",
    "\n",
    "                # warnings.filterwarnings('default')\n",
    "\n",
    "            result_table = pd.DataFrame(results)\n",
    "            result_table.columns = ['parameters', 'aic']\n",
    "            result_table = result_table.sort_values(by = 'aic', ascending=True)[:1]\n",
    "            result_table['store_nbr'] = store\n",
    "            result_table['family'] = fam\n",
    "            all_result_table= all_result_table.append(result_table, ignore_index=True)\n",
    "            print('best param: ', result_table)\n",
    "            best_model = sm.tsa.arima.ARIMA(a.sales, order=(param[0], d, param[1])).fit()#, seasonal_order=(param[3], D, param[3], 24*7)).fit(disp=0)\n",
    "            # d_train.loc[(d_train.store_nbr == store) & (d_train.family == fam),\"arima_model\"] = np.array(best_model.fittedvalues)\n",
    "            d_test.loc[(d_test.store_nbr == store) & (d_test.family == fam),'arima_model']=np.array(best_model.predict(start = a.shape[0], end = a.shape[0]+15))\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "print(all_result_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit for ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = d_test.set_index(['store_nbr', 'family', 'date']).sort_index()\n",
    "for i in zero_prediction.index:\n",
    "    d_test.loc[i,'arima_model']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = d_test.reset_index()\n",
    "y_submit = d_test[['id','arima_model']].copy()\n",
    "y_submit.columns = ['id', 'sales']\n",
    "y_submit.to_csv('submission_arima_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель Хольта-Винтерса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, store in enumerate(tqdm(d_train.store_nbr.unique())):\n",
    "    for numf, fam in enumerate(tqdm(d_train.family.unique())):\n",
    "        a = d_train[(d_train.store_nbr == store) & (d_train.family == fam)].copy()\n",
    "        \n",
    "        if len(a)!=0:\n",
    "            model_HW = ExponentialSmoothing(a.sales, seasonal_periods=7, trend='add', seasonal='add').fit()\n",
    "            # d_train.loc[(d_train.store_nbr == store) & (d_train.family == fam),'HW_pred']=np.array(model_HW.fittedvalues)\n",
    "            d_test.loc[(d_test.store_nbr == store) & (d_test.family == fam),'HW_pred']=np.array(model_HW.forecast(16))\n",
    "            #print('store:', store, 'fam:', fam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit for Holt-Winters model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_test = d_test.reset_index().set_index(['store_nbr', 'family', 'date']).sort_index()\n",
    "d_test = d_test.set_index(['store_nbr', 'family', 'date']).sort_index()\n",
    "for i in zero_prediction.index:\n",
    "    d_test.loc[i,'HW_pred']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test=d_test.reset_index()\n",
    "HW_submit = d_test[['id','HW_pred']].copy()\n",
    "HW_submit.columns = ['id', 'sales']\n",
    "HW_submit.to_csv('submission_HW.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_submit.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результат на kaggle 0.43267**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем отступление от принятой модели вычисления предсказания и возьмем среднее по дням вне зависимостри от семейства или магазина. Изучим полученный ряд на тренд и сезонность, построим периодограмму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренд для обобщенного ряда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"ignore\")  # ignore warnings to clean up output cells\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True, figsize=(11, 5))\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Группируем по дню \n",
    "avg_sales = d_train.groupby('date').agg({'sales': 'mean'}).reset_index()\n",
    "avg_sales = avg_sales.set_index('date').to_period(\"D\")\n",
    "\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "    index=avg_sales.index,  # dates from the training data\n",
    "    constant=True,       # dummy feature for the bias (y_intercept)\n",
    "    order=1,             # the time dummy (trend)\n",
    "    drop=True,           # drop terms if necessary to avoid collinearity\n",
    ")\n",
    "# `in_sample` creates features for the dates given in the `index` argument\n",
    "X = dp.in_sample()\n",
    "y = avg_sales[\"sales\"]  # the target\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "y_pred = pd.Series(model.predict(X), index=X.index)\n",
    "\n",
    "ax = avg_sales.plot(style=\".\", color=\"0.5\", title=\"sales - Linear Trend\")\n",
    "_ = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сезонность для обобщенного ряда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_plot(X, y, period, freq, ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n",
    "    ax = sns.lineplot(\n",
    "        x=freq,\n",
    "        y=y,\n",
    "        hue=period,\n",
    "        data=X,\n",
    "        ci=False,\n",
    "        ax=ax,\n",
    "        palette=palette,\n",
    "        legend=False,\n",
    "    )\n",
    "    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n",
    "    for line, name in zip(ax.lines, X[period].unique()):\n",
    "        y_ = line.get_ydata()[-1]\n",
    "        ax.annotate(\n",
    "            name,\n",
    "            xy=(1, y_),\n",
    "            xytext=(6, 0),\n",
    "            color=line.get_color(),\n",
    "            xycoords=ax.get_yaxis_transform(),\n",
    "            textcoords=\"offset points\",\n",
    "            size=14,\n",
    "            va=\"center\",\n",
    "        )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_periodogram(ts, detrend='linear', ax=None):\n",
    "    from scipy.signal import periodogram\n",
    "    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n",
    "    freqencies, spectrum = periodogram(\n",
    "        ts,\n",
    "        fs=fs,\n",
    "        detrend=detrend,\n",
    "        window=\"boxcar\",\n",
    "        scaling='spectrum',\n",
    "    )\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.step(freqencies, spectrum, color=\"purple\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n",
    "    ax.set_xticklabels(\n",
    "        [\n",
    "            \"Annual (1)\",\n",
    "            \"Semiannual (2)\",\n",
    "            \"Quarterly (4)\",\n",
    "            \"Bimonthly (6)\",\n",
    "            \"Monthly (12)\",\n",
    "            \"Biweekly (26)\",\n",
    "            \"Weekly (52)\",\n",
    "            \"Semiweekly (104)\",\n",
    "        ],\n",
    "        rotation=30,\n",
    "    )\n",
    "    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "    ax.set_ylabel(\"Variance\")\n",
    "    ax.set_title(\"Periodogram\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_timefit = avg_sales.copy()\n",
    "\n",
    "# days within a week\n",
    "X_timefit['day'] = X_timefit.index.dayofweek # the x-axis (freq)\n",
    "X_timefit['week'] = X_timefit.index.week # the seasonal period (period)\n",
    "\n",
    "# days within a year\n",
    "X_timefit['dayofyear'] = X_timefit.index.dayofyear\n",
    "X_timefit['year'] = X_timefit.index.year\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6))\n",
    "seasonal_plot(X_timefit, y=\"sales\", period=\"week\", freq=\"day\", ax=ax0)\n",
    "seasonal_plot(X_timefit, y=\"sales\", period=\"year\", freq=\"dayofyear\", ax=ax1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_detrend = y - y_pred \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 7))\n",
    "ax1 = plot_periodogram(y, ax=ax1)\n",
    "ax1.set_title(\"Product Sales Frequency Components\")\n",
    "ax2 = plot_periodogram(y_detrend, ax=ax2);\n",
    "ax2.set_title(\"Detrended\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прослеживается сильный недельный сезон и более слабый годовой сезон. Недельный сезон мы будем моделировать с помощью индикаторов, а годовой сезон — с помощью функций Фурье. Периодограмма затихает между раз Bimonthly (6) и Monthly (12),будем используем 10 пар Фурье."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный рузельтат будем применять в разрезе магазинов и семейств товаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В нашем датасете остались категориальные признаки,\n",
    "# которые необходимо закодировать в числовые.\n",
    "\n",
    "lab_enc_col = ['city','state','type','cluster']\n",
    "# Label Encoding\n",
    "for column in lab_enc_col:\n",
    "    df[column] = df[column].astype('category').cat.codes\n",
    "        \n",
    "# One-Hot Encoding c get_dummies.\n",
    "df = pd.get_dummies(df, columns=lab_enc_col, dummy_na=False)\n",
    "\n",
    "# Раздеряем на test и train\n",
    "d_train = df[df.date<'2017-08-16'].copy()\n",
    "d_test = df[df.date>='2017-08-16'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test['lr_model'] = 0\n",
    "fourier = CalendarFourier(freq=\"A\", order=10) \n",
    "for num, store in enumerate(tqdm(d_train.store_nbr.unique())):\n",
    "    for numf, fam in enumerate(tqdm(d_train.family.unique())):\n",
    "        a = d_train[(d_train.store_nbr == store) & (d_train.family == fam)].copy()\n",
    "        a = a.set_index('date').to_period(\"D\")\n",
    "        b = str(store)+','+fam\n",
    "        \n",
    "        if len(a)!=0:\n",
    "            if b in s_fuller_list:\n",
    "                a.sales = a.sales.diff().fillna(0)\n",
    "                \n",
    "            dp = DeterministicProcess(\n",
    "                index=a.index,\n",
    "                constant=True,   # dummy feature for bias (y-intercept)\n",
    "                order=1,         # trend ( order 1 means linear)\n",
    "                seasonal=True,   # weekly seasonality (indicators)\n",
    "                additional_terms=[fourier], # annual seasonality\n",
    "                drop=True,       # drop terms to avoid collinearity\n",
    "            )\n",
    "\n",
    "            X = dp.in_sample() # create features for dates in tunnel.index\n",
    "            \n",
    "            X = X.join(a.drop(['family','store_nbr','id','sales','year'],axis=1))\n",
    "            y = a.sales\n",
    "\n",
    "            model = LinearRegression(fit_intercept=False)\n",
    "            model.fit(X, y)\n",
    "\n",
    "            X_fore = dp.out_of_sample(16)\n",
    "            a_fore = d_test[(d_test.store_nbr == store) & (d_test.family == fam)].copy()\n",
    "            a_fore = a_fore.set_index('date').to_period(\"D\")\n",
    "            X_fore = X_fore.join(a_fore.drop(['family','store_nbr','id','sales','year','lr_model'],axis=1))\n",
    "            d_test.loc[(d_test.store_nbr == store) & (d_test.family == fam), 'lr_model'] = model.predict(X_fore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = d_test.set_index(['store_nbr', 'family', 'date']).sort_index()\n",
    "for i in zero_prediction.index:\n",
    "    d_test.loc[i,'lr_model']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test=d_test.reset_index()\n",
    "lr_submit = d_test[['id','lr_model']].copy()\n",
    "lr_submit.columns = ['id', 'sales']\n",
    "lr_submit.to_csv('submission_lr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_submit.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test['xgb_model'] = 0\n",
    "fourier = CalendarFourier(freq=\"A\", order=10) \n",
    "for num, store in enumerate(tqdm(d_train.store_nbr.unique())):\n",
    "    for numf, fam in enumerate(tqdm(d_train.family.unique())):\n",
    "        a = d_train[(d_train.store_nbr == store) & (d_train.family == fam)].copy()\n",
    "        a = a.set_index('date').to_period(\"D\")\n",
    "        b = str(store)+','+fam\n",
    "        \n",
    "        if len(a)!=0:\n",
    "            if b in s_fuller_list:\n",
    "                a.sales = a.sales.diff().fillna(0)\n",
    "                \n",
    "            dp = DeterministicProcess(\n",
    "                index=a.index,\n",
    "                constant=True,   # dummy feature for bias (y-intercept)\n",
    "                order=1,         # trend ( order 1 means linear)\n",
    "                seasonal=True,   # weekly seasonality (indicators)\n",
    "                additional_terms=[fourier], # annual seasonality\n",
    "                drop=True,       # drop terms to avoid collinearity\n",
    "            )\n",
    "\n",
    "            X = dp.in_sample() # create features for dates in tunnel.index\n",
    "            X_train = X.join(a.drop(['family','store_nbr','id','sales','year'],axis=1))# !!! удалить lr_model\n",
    "            y = a.sales\n",
    "            \n",
    "            X_fore = dp.out_of_sample(16)\n",
    "            a_fore = d_test[(d_test.store_nbr == store) & (d_test.family == fam)].copy()\n",
    "            a_fore = a_fore.set_index('date').to_period(\"D\")\n",
    "            X_fore = X_fore.join(a_fore.drop(['family','store_nbr','id','sales','year','xgb_model'],axis=1)) # !!!\n",
    "            \n",
    "            dtrain = xgb.DMatrix(X_train, label=y)\n",
    "            dtest = xgb.DMatrix(X_fore)\n",
    "            \n",
    "            # задаём параметры\n",
    "            params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'booster':'gblinear'\n",
    "            }\n",
    "            trees = 1000\n",
    "            \n",
    "            # прогоняем на кросс-валидации с метрикой rmse\n",
    "            cv = xgb.cv(params, dtrain, metrics = ('rmse'), verbose_eval=False, nfold=10, show_stdv=False, num_boost_round=trees)\n",
    "            print('store: ', store, 'fam: ', fam)\n",
    "            # обучаем xgboost с оптимальным числом деревьев, подобранным на кросс-валидации\n",
    "            bst = xgb.train(params, dtrain, num_boost_round=cv['test-rmse-mean'].argmin())\n",
    "\n",
    "            d_test.loc[(d_test.store_nbr == store) & (d_test.family == fam), 'xgb_model'] = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = d_test.set_index(['store_nbr', 'family', 'date']).sort_index()\n",
    "for i in zero_prediction.index:\n",
    "    d_test.loc[i,'xgb_model']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test=d_test.reset_index()\n",
    "xgb_submit = d_test[['id','xgb_model']].copy()\n",
    "xgb_submit.columns = ['id', 'sales']\n",
    "xgb_submit.to_csv('submission_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_submit.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
